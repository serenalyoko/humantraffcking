{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc954552-dadd-48b7-9b94-f1525049e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_path =  os.getcwd() + '/htmls/nychinaren_url/'\n",
    "image_src_input_path_train = os.getcwd() + '/image_phone_data/training_data/image_urls.txt'\n",
    "label_input_path_train = os.getcwd() + '/image_phone_data/training_data/image_labels.txt'\n",
    "image_src_input_path = os.getcwd() + '/image_phone_data/real_data/image_urls.txt'\n",
    "label_input_path = os.getcwd() + '/image_phone_data/real_data/image_labels.txt'\n",
    "model_path = os.getcwd() + '/image_phone_data/model/'\n",
    "all_files = os.listdir(html_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c0f790a-f35e-40f5-b3de-ede991d2822d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Configure models\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, EarlyStoppingCallback\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-small-printed\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-small-printed\").to(device)\n",
    "\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "\n",
    "# Adjust these?\n",
    "#model.config.num_beams = 4 \n",
    "#model.config.early_stopping = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbf77058-51d8-48f8-bbe0-68429c9b4493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimg_srcs = []\\n\\nfor fn in all_files:\\n    with open(html_path + fn, \"r\") as f:\\n        html_content = f.read()\\n        soup = BeautifulSoup(html_content, \\'html.parser\\')\\n            \\n        contact_img_divs = soup.select(\\'div.frm_rent\\')\\n        for div in contact_img_divs:\\n            if \\'frm_phone\\' in div[\\'class\\']:\\n                img_src = div.find(\\'img\\')[\\'src\\']\\n                img_srcs.append(img_src)\\n\\nwith open(image_src_input_path, \"w\") as f:\\n    for src in img_srcs:\\n        f.write(src + \"\\n\")\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save image sources into txt\n",
    "\"\"\"\n",
    "img_srcs = []\n",
    "\n",
    "for fn in all_files:\n",
    "    with open(html_path + fn, \"r\") as f:\n",
    "        html_content = f.read()\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            \n",
    "        contact_img_divs = soup.select('div.frm_rent')\n",
    "        for div in contact_img_divs:\n",
    "            if 'frm_phone' in div['class']:\n",
    "                img_src = div.find('img')['src']\n",
    "                img_srcs.append(img_src)\n",
    "\n",
    "with open(image_src_input_path, \"w\") as f:\n",
    "    for src in img_srcs:\n",
    "        f.write(src + \"\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22a5c705-6256-4c6a-970c-45e36fe7a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr(src, processor, model):\n",
    "    img = cv2.imdecode(np.asarray(bytearray(requests.get(src, stream=True).content), dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary = cv2.threshold(img, 200, 255, cv2.THRESH_BINARY)\n",
    "    img = cv2.bitwise_not(binary)\n",
    "    img = cv2.dilate(img, np.ones((2, 2), np.uint8), iterations=1)\n",
    "    img = cv2.bitwise_not(img)\n",
    "    #img = cv2.copyMakeBorder(img, 20, 20, 0, 0, cv2.BORDER_CONSTANT, value=[255, 255, 255])    \n",
    "    img = cv2.copyMakeBorder(img, 20, 20, 20, 20, cv2.BORDER_CONSTANT, value=[255, 255, 255])    \n",
    "    \n",
    "    image = Image.fromarray(img).convert(\"RGB\")\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc0a1688-2158-4f9b-a151-cb06838a494a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(image_src_input_path_train, \"r\") as img_srcs, open(label_input_path_train, \"w\") as f:  \\n    for src in img_srcs:        \\n        generated_text = ocr(src.strip(), processor, model)\\n        f.write(generated_text + \"\\n\")\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Form some initial labels to help with manual labeling\n",
    "\"\"\"\n",
    "with open(image_src_input_path_train, \"r\") as img_srcs, open(label_input_path_train, \"w\") as f:  \n",
    "    for src in img_srcs:        \n",
    "        generated_text = ocr(src.strip(), processor, model)\n",
    "        f.write(generated_text + \"\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1be801af-6351-4251-8c1b-6f434975be25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_src</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://ny.nychinaren.com/images/topic_info/33...</td>\n",
       "      <td>917-436-9760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://ny.nychinaren.com/images/topic_info/33...</td>\n",
       "      <td>3478635699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://ny.nychinaren.com/images/topic_info/33...</td>\n",
       "      <td>212-672-6486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://ny.nychinaren.com/images/topic_info/33...</td>\n",
       "      <td>6463547367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://ny.nychinaren.com/images/topic_info/33...</td>\n",
       "      <td>6464009707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>https://ny.nychinaren.com/images/topic_info/33...</td>\n",
       "      <td>631-231-8999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>https://ny.nychinaren.com/images/topic_info/33...</td>\n",
       "      <td>6173814478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>https://ny.nychinaren.com/images/topic_info/33...</td>\n",
       "      <td>(347) 884-2610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>https://ny.nychinaren.com/images/topic_info/33...</td>\n",
       "      <td>347-838-1938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>https://ny.nychinaren.com/images/topic_info/33...</td>\n",
       "      <td>7188665670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               img_src           label\n",
       "0    https://ny.nychinaren.com/images/topic_info/33...    917-436-9760\n",
       "1    https://ny.nychinaren.com/images/topic_info/33...      3478635699\n",
       "2    https://ny.nychinaren.com/images/topic_info/33...    212-672-6486\n",
       "3    https://ny.nychinaren.com/images/topic_info/33...      6463547367\n",
       "4    https://ny.nychinaren.com/images/topic_info/33...      6464009707\n",
       "..                                                 ...             ...\n",
       "218  https://ny.nychinaren.com/images/topic_info/33...    631-231-8999\n",
       "219  https://ny.nychinaren.com/images/topic_info/33...      6173814478\n",
       "220  https://ny.nychinaren.com/images/topic_info/33...  (347) 884-2610\n",
       "221  https://ny.nychinaren.com/images/topic_info/33...    347-838-1938\n",
       "222  https://ny.nychinaren.com/images/topic_info/33...      7188665670\n",
       "\n",
       "[223 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the txt files into a dataframe\n",
    "df_rows = []\n",
    "\n",
    "with open(image_src_input_path_train, \"r\") as img_src_fn, open(label_input_path_train, \"r\") as label_fn:\n",
    "    for line1, line2 in zip(img_src_fn, label_fn):\n",
    "        df_rows.append((line1.strip(), line2.strip()))\n",
    "\n",
    "data_df = pd.DataFrame(df_rows, columns=['img_src', 'label'])\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6c993be-35e5-4ece-9039-2c03add179e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch dataset class based on our dataframe\n",
    "class SkewedDigitsDataset(Dataset):\n",
    "    def __init__(self, df, processor, max_target_length=20):        \n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        img_src = self.df['img_src'][idx]\n",
    "        text = self.df['label'][idx]\n",
    "\n",
    "        \n",
    "        # Image preprocessing for better accuracy (convert to binary, dilate, add border) and get pixel values from processor\n",
    "        img = cv2.imdecode(np.asarray(bytearray(requests.get(img_src, stream=True).content), dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "        \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        _, binary = cv2.threshold(img, 200, 255, cv2.THRESH_BINARY)\n",
    "        img = cv2.bitwise_not(binary)\n",
    "        img = cv2.dilate(img, np.ones((2, 2), np.uint8), iterations=1)\n",
    "        img = cv2.bitwise_not(img)\n",
    "        #img = cv2.copyMakeBorder(img, 20, 20, 0, 0, cv2.BORDER_CONSTANT, value=[255, 255, 255])\n",
    "        img = cv2.copyMakeBorder(img, 20, 20, 20, 20, cv2.BORDER_CONSTANT, value=[255, 255, 255])\n",
    "        \n",
    "        img = Image.fromarray(img).convert(\"RGB\")\n",
    "        pixel_values = self.processor(img, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        # Add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(text, \n",
    "                                          padding=\"max_length\", \n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        # Important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38247a6f-9724-4b3c-b7b0-d8208124ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training and test sets\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.2, random_state=42)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "training_set = SkewedDigitsDataset(train_df, processor)\n",
    "validation_set = SkewedDigitsDataset(test_df, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb5b121a-aadb-4e4c-9269-ad1702a16a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2777/2071459853.py:6: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  cer_metric = load_metric('cer')\n",
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for cer contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/cer/cer.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_metric\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4) # Change learning rate?\n",
    "cer_metric = load_metric('cer')\n",
    "def compute_cer(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    " \n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    " \n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee451e8-7684-4f7a-8a89-c02dc6a64c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/transformers/models/trocr/processing_trocr.py:136: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    fp16=True,\n",
    "    output_dir=model_path,\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=5,    \n",
    "    num_train_epochs=20,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_cer,\n",
    "    train_dataset=training_set,\n",
    "    eval_dataset=validation_set,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.03)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91fd755f-1ac4-4129-b207-a2682e1f8ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='368' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [368/460 17:23 < 04:22, 0.35 it/s, Epoch 16/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.782200</td>\n",
       "      <td>4.516408</td>\n",
       "      <td>0.861446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.163700</td>\n",
       "      <td>3.878091</td>\n",
       "      <td>1.138554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.564900</td>\n",
       "      <td>3.484702</td>\n",
       "      <td>0.835341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.045100</td>\n",
       "      <td>2.618145</td>\n",
       "      <td>0.594378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.539400</td>\n",
       "      <td>1.275967</td>\n",
       "      <td>0.184739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.657100</td>\n",
       "      <td>0.982950</td>\n",
       "      <td>0.301205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.547400</td>\n",
       "      <td>0.939613</td>\n",
       "      <td>0.146586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.410500</td>\n",
       "      <td>0.946572</td>\n",
       "      <td>0.232932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.311600</td>\n",
       "      <td>0.744497</td>\n",
       "      <td>0.138554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.282600</td>\n",
       "      <td>0.697640</td>\n",
       "      <td>0.253012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.565387</td>\n",
       "      <td>0.070281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.582549</td>\n",
       "      <td>0.072289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.136800</td>\n",
       "      <td>0.533691</td>\n",
       "      <td>0.094378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.164300</td>\n",
       "      <td>0.640108</td>\n",
       "      <td>0.068273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.090600</td>\n",
       "      <td>0.521909</td>\n",
       "      <td>0.046185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.095600</td>\n",
       "      <td>0.585866</td>\n",
       "      <td>0.084337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=368, training_loss=1.518221045317857, metrics={'train_runtime': 1047.2449, 'train_samples_per_second': 3.399, 'train_steps_per_second': 0.439, 'total_flos': 3.406911097704284e+17, 'train_loss': 1.518221045317857, 'epoch': 16.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "# 0.3527, 0.53119, 0.236948 was the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b7e27a2-29f3-457b-b312-530afd08f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_path + \"/trained_model_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbeb419d-5f0f-45a8-aa1c-66de23e8379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-small-printed\")\n",
    "trained_model = VisionEncoderDecoderModel.from_pretrained(model_path + \"/trained_model_test\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15891db3-dfd9-4994-9e48-fbeb8e3c0255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/notrichardpeng/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open(image_src_input_path, \"r\") as img_srcs, open(label_input_path, \"w\") as f:  \n",
    "    for src in img_srcs:                     \n",
    "        generated_text = ocr(src.strip(), trained_processor, trained_model)        \n",
    "        f.write(generated_text + \"\\n\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f003f236-8399-4425-ae42-06fe3fe20f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
